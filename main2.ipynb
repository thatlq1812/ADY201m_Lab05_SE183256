{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path \n",
    "folder_path = 'D:\\FPTUni\\SP24\\ADY201m\\Lab05\\ADY201m_Lab05_SE183256'\n",
    "submit_path = 'D:\\FPTUni\\SP24\\ADY201m\\Lab05\\ADY201m_Lab05_SE183256'\n",
    "# Import the dataset\n",
    "train_data = pd.read_csv(os.path.join(folder_path, 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'))\n",
    "submit_file = os.path.join(folder_path, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "threshold = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because MSSubClass is int64 but it is a categorical variable, so we need to convert it to string\n",
    "train_data['MSSubClass'] = train_data['MSSubClass'].astype(str)\n",
    "# Drop Id column\n",
    "train_data.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical columns\n",
    "categorical_cols = train_data.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical columns, keep dataframes type\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    train_data[col] = label_encoder.fit_transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix with SalePrice\n",
    "correlation_matrix = train_data.corrwith(train_data['SalePrice']).sort_values(ascending=False)\n",
    "# Print the correlation matrix\n",
    "# ABS correlation matrix and choose the features have correlation >= threshold\n",
    "selected_features = correlation_matrix[abs(correlation_matrix) >= threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features to new dataframe\n",
    "prelast_train_data = train_data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo scaler\n",
    "scaler = StandardScaler()\n",
    "# Build scaler dành riêng cho tập output\n",
    "scaler_output = StandardScaler()\n",
    "scaler_output.fit(prelast_train_data['SalePrice'].values.reshape(-1, 1))\n",
    "# Fit scaler trên tập train\n",
    "scaler.fit(prelast_train_data)\n",
    "# Transform tập train\n",
    "last_train_data = scaler.transform(prelast_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_2440\\1217171245.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  last_train_data = torch.tensor(last_train_data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to torch tensor\n",
    "torch.set_printoptions(precision=10)\n",
    "last_train_data = torch.tensor(last_train_data, dtype=torch.float32)\n",
    "\n",
    "# Prepare the training set, last_train_data is the scaled training set and it is a tensor now\n",
    "X_pretrain = last_train_data[:, 1:]\n",
    "y_pretrain = last_train_data[:, 0]\n",
    "\n",
    "# Split the train data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pretrain, y_pretrain, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))  # Lớp ẩn 1 với hàm kích hoạt ReLU\n",
    "        x = self.relu(self.fc2(x))  # Lớp ẩn 2 với hàm kích hoạt ReLU\n",
    "        x = self.sigmoid(self.fc3(x))  # Lớp đầu ra với hàm kích hoạt Sigmoid\n",
    "        return x\n",
    "\n",
    "# Dữ liệu mẫu\n",
    "input_size = 14  # Số lượng đặc trưng đầu vào\n",
    "hidden_size1 = 16  # Số lượng units trong lớp ẩn 1\n",
    "hidden_size2 = 8  # Số lượng units trong lớp ẩn 2\n",
    "output_size = 1  # Số lượng output (trong bài toán hồi quy)\n",
    "\n",
    "# Khởi tạo mô hình MLP\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 1.1645\n",
      "Epoch [20/5000], Loss: 1.1525\n",
      "Epoch [30/5000], Loss: 1.1406\n",
      "Epoch [40/5000], Loss: 1.1288\n",
      "Epoch [50/5000], Loss: 1.1170\n",
      "Epoch [60/5000], Loss: 1.1052\n",
      "Epoch [70/5000], Loss: 1.0934\n",
      "Epoch [80/5000], Loss: 1.0815\n",
      "Epoch [90/5000], Loss: 1.0696\n",
      "Epoch [100/5000], Loss: 1.0577\n",
      "Epoch [110/5000], Loss: 1.0458\n",
      "Epoch [120/5000], Loss: 1.0338\n",
      "Epoch [130/5000], Loss: 1.0220\n",
      "Epoch [140/5000], Loss: 1.0103\n",
      "Epoch [150/5000], Loss: 0.9988\n",
      "Epoch [160/5000], Loss: 0.9873\n",
      "Epoch [170/5000], Loss: 0.9761\n",
      "Epoch [180/5000], Loss: 0.9651\n",
      "Epoch [190/5000], Loss: 0.9544\n",
      "Epoch [200/5000], Loss: 0.9441\n",
      "Epoch [210/5000], Loss: 0.9342\n",
      "Epoch [220/5000], Loss: 0.9246\n",
      "Epoch [230/5000], Loss: 0.9155\n",
      "Epoch [240/5000], Loss: 0.9069\n",
      "Epoch [250/5000], Loss: 0.8987\n",
      "Epoch [260/5000], Loss: 0.8910\n",
      "Epoch [270/5000], Loss: 0.8837\n",
      "Epoch [280/5000], Loss: 0.8769\n",
      "Epoch [290/5000], Loss: 0.8706\n",
      "Epoch [300/5000], Loss: 0.8646\n",
      "Epoch [310/5000], Loss: 0.8589\n",
      "Epoch [320/5000], Loss: 0.8537\n",
      "Epoch [330/5000], Loss: 0.8487\n",
      "Epoch [340/5000], Loss: 0.8441\n",
      "Epoch [350/5000], Loss: 0.8397\n",
      "Epoch [360/5000], Loss: 0.8355\n",
      "Epoch [370/5000], Loss: 0.8316\n",
      "Epoch [380/5000], Loss: 0.8279\n",
      "Epoch [390/5000], Loss: 0.8244\n",
      "Epoch [400/5000], Loss: 0.8211\n",
      "Epoch [410/5000], Loss: 0.8179\n",
      "Epoch [420/5000], Loss: 0.8148\n",
      "Epoch [430/5000], Loss: 0.8119\n",
      "Epoch [440/5000], Loss: 0.8091\n",
      "Epoch [450/5000], Loss: 0.8064\n",
      "Epoch [460/5000], Loss: 0.8038\n",
      "Epoch [470/5000], Loss: 0.8013\n",
      "Epoch [480/5000], Loss: 0.7989\n",
      "Epoch [490/5000], Loss: 0.7965\n",
      "Epoch [500/5000], Loss: 0.7942\n",
      "Epoch [510/5000], Loss: 0.7919\n",
      "Epoch [520/5000], Loss: 0.7897\n",
      "Epoch [530/5000], Loss: 0.7875\n",
      "Epoch [540/5000], Loss: 0.7854\n",
      "Epoch [550/5000], Loss: 0.7833\n",
      "Epoch [560/5000], Loss: 0.7812\n",
      "Epoch [570/5000], Loss: 0.7791\n",
      "Epoch [580/5000], Loss: 0.7771\n",
      "Epoch [590/5000], Loss: 0.7750\n",
      "Epoch [600/5000], Loss: 0.7730\n",
      "Epoch [610/5000], Loss: 0.7709\n",
      "Epoch [620/5000], Loss: 0.7689\n",
      "Epoch [630/5000], Loss: 0.7668\n",
      "Epoch [640/5000], Loss: 0.7647\n",
      "Epoch [650/5000], Loss: 0.7626\n",
      "Epoch [660/5000], Loss: 0.7605\n",
      "Epoch [670/5000], Loss: 0.7583\n",
      "Epoch [680/5000], Loss: 0.7561\n",
      "Epoch [690/5000], Loss: 0.7539\n",
      "Epoch [700/5000], Loss: 0.7516\n",
      "Epoch [710/5000], Loss: 0.7493\n",
      "Epoch [720/5000], Loss: 0.7469\n",
      "Epoch [730/5000], Loss: 0.7445\n",
      "Epoch [740/5000], Loss: 0.7420\n",
      "Epoch [750/5000], Loss: 0.7394\n",
      "Epoch [760/5000], Loss: 0.7368\n",
      "Epoch [770/5000], Loss: 0.7341\n",
      "Epoch [780/5000], Loss: 0.7313\n",
      "Epoch [790/5000], Loss: 0.7285\n",
      "Epoch [800/5000], Loss: 0.7255\n",
      "Epoch [810/5000], Loss: 0.7225\n",
      "Epoch [820/5000], Loss: 0.7193\n",
      "Epoch [830/5000], Loss: 0.7161\n",
      "Epoch [840/5000], Loss: 0.7127\n",
      "Epoch [850/5000], Loss: 0.7093\n",
      "Epoch [860/5000], Loss: 0.7058\n",
      "Epoch [870/5000], Loss: 0.7022\n",
      "Epoch [880/5000], Loss: 0.6985\n",
      "Epoch [890/5000], Loss: 0.6947\n",
      "Epoch [900/5000], Loss: 0.6909\n",
      "Epoch [910/5000], Loss: 0.6870\n",
      "Epoch [920/5000], Loss: 0.6831\n",
      "Epoch [930/5000], Loss: 0.6791\n",
      "Epoch [940/5000], Loss: 0.6752\n",
      "Epoch [950/5000], Loss: 0.6713\n",
      "Epoch [960/5000], Loss: 0.6674\n",
      "Epoch [970/5000], Loss: 0.6637\n",
      "Epoch [980/5000], Loss: 0.6599\n",
      "Epoch [990/5000], Loss: 0.6563\n",
      "Epoch [1000/5000], Loss: 0.6529\n",
      "Epoch [1010/5000], Loss: 0.6495\n",
      "Epoch [1020/5000], Loss: 0.6463\n",
      "Epoch [1030/5000], Loss: 0.6432\n",
      "Epoch [1040/5000], Loss: 0.6403\n",
      "Epoch [1050/5000], Loss: 0.6375\n",
      "Epoch [1060/5000], Loss: 0.6348\n",
      "Epoch [1070/5000], Loss: 0.6323\n",
      "Epoch [1080/5000], Loss: 0.6299\n",
      "Epoch [1090/5000], Loss: 0.6277\n",
      "Epoch [1100/5000], Loss: 0.6256\n",
      "Epoch [1110/5000], Loss: 0.6236\n",
      "Epoch [1120/5000], Loss: 0.6217\n",
      "Epoch [1130/5000], Loss: 0.6199\n",
      "Epoch [1140/5000], Loss: 0.6182\n",
      "Epoch [1150/5000], Loss: 0.6166\n",
      "Epoch [1160/5000], Loss: 0.6150\n",
      "Epoch [1170/5000], Loss: 0.6136\n",
      "Epoch [1180/5000], Loss: 0.6122\n",
      "Epoch [1190/5000], Loss: 0.6108\n",
      "Epoch [1200/5000], Loss: 0.6096\n",
      "Epoch [1210/5000], Loss: 0.6083\n",
      "Epoch [1220/5000], Loss: 0.6072\n",
      "Epoch [1230/5000], Loss: 0.6060\n",
      "Epoch [1240/5000], Loss: 0.6049\n",
      "Epoch [1250/5000], Loss: 0.6039\n",
      "Epoch [1260/5000], Loss: 0.6029\n",
      "Epoch [1270/5000], Loss: 0.6020\n",
      "Epoch [1280/5000], Loss: 0.6010\n",
      "Epoch [1290/5000], Loss: 0.6001\n",
      "Epoch [1300/5000], Loss: 0.5993\n",
      "Epoch [1310/5000], Loss: 0.5985\n",
      "Epoch [1320/5000], Loss: 0.5977\n",
      "Epoch [1330/5000], Loss: 0.5970\n",
      "Epoch [1340/5000], Loss: 0.5962\n",
      "Epoch [1350/5000], Loss: 0.5955\n",
      "Epoch [1360/5000], Loss: 0.5948\n",
      "Epoch [1370/5000], Loss: 0.5941\n",
      "Epoch [1380/5000], Loss: 0.5935\n",
      "Epoch [1390/5000], Loss: 0.5929\n",
      "Epoch [1400/5000], Loss: 0.5923\n",
      "Epoch [1410/5000], Loss: 0.5917\n",
      "Epoch [1420/5000], Loss: 0.5911\n",
      "Epoch [1430/5000], Loss: 0.5905\n",
      "Epoch [1440/5000], Loss: 0.5900\n",
      "Epoch [1450/5000], Loss: 0.5895\n",
      "Epoch [1460/5000], Loss: 0.5889\n",
      "Epoch [1470/5000], Loss: 0.5884\n",
      "Epoch [1480/5000], Loss: 0.5879\n",
      "Epoch [1490/5000], Loss: 0.5874\n",
      "Epoch [1500/5000], Loss: 0.5870\n",
      "Epoch [1510/5000], Loss: 0.5865\n",
      "Epoch [1520/5000], Loss: 0.5860\n",
      "Epoch [1530/5000], Loss: 0.5856\n",
      "Epoch [1540/5000], Loss: 0.5852\n",
      "Epoch [1550/5000], Loss: 0.5847\n",
      "Epoch [1560/5000], Loss: 0.5843\n",
      "Epoch [1570/5000], Loss: 0.5839\n",
      "Epoch [1580/5000], Loss: 0.5835\n",
      "Epoch [1590/5000], Loss: 0.5831\n",
      "Epoch [1600/5000], Loss: 0.5827\n",
      "Epoch [1610/5000], Loss: 0.5824\n",
      "Epoch [1620/5000], Loss: 0.5820\n",
      "Epoch [1630/5000], Loss: 0.5816\n",
      "Epoch [1640/5000], Loss: 0.5813\n",
      "Epoch [1650/5000], Loss: 0.5810\n",
      "Epoch [1660/5000], Loss: 0.5806\n",
      "Epoch [1670/5000], Loss: 0.5803\n",
      "Epoch [1680/5000], Loss: 0.5800\n",
      "Epoch [1690/5000], Loss: 0.5796\n",
      "Epoch [1700/5000], Loss: 0.5793\n",
      "Epoch [1710/5000], Loss: 0.5790\n",
      "Epoch [1720/5000], Loss: 0.5787\n",
      "Epoch [1730/5000], Loss: 0.5784\n",
      "Epoch [1740/5000], Loss: 0.5781\n",
      "Epoch [1750/5000], Loss: 0.5779\n",
      "Epoch [1760/5000], Loss: 0.5776\n",
      "Epoch [1770/5000], Loss: 0.5773\n",
      "Epoch [1780/5000], Loss: 0.5770\n",
      "Epoch [1790/5000], Loss: 0.5768\n",
      "Epoch [1800/5000], Loss: 0.5765\n",
      "Epoch [1810/5000], Loss: 0.5762\n",
      "Epoch [1820/5000], Loss: 0.5760\n",
      "Epoch [1830/5000], Loss: 0.5757\n",
      "Epoch [1840/5000], Loss: 0.5755\n",
      "Epoch [1850/5000], Loss: 0.5753\n",
      "Epoch [1860/5000], Loss: 0.5750\n",
      "Epoch [1870/5000], Loss: 0.5748\n",
      "Epoch [1880/5000], Loss: 0.5745\n",
      "Epoch [1890/5000], Loss: 0.5743\n",
      "Epoch [1900/5000], Loss: 0.5741\n",
      "Epoch [1910/5000], Loss: 0.5739\n",
      "Epoch [1920/5000], Loss: 0.5736\n",
      "Epoch [1930/5000], Loss: 0.5734\n",
      "Epoch [1940/5000], Loss: 0.5732\n",
      "Epoch [1950/5000], Loss: 0.5730\n",
      "Epoch [1960/5000], Loss: 0.5728\n",
      "Epoch [1970/5000], Loss: 0.5726\n",
      "Epoch [1980/5000], Loss: 0.5724\n",
      "Epoch [1990/5000], Loss: 0.5722\n",
      "Epoch [2000/5000], Loss: 0.5720\n",
      "Epoch [2010/5000], Loss: 0.5718\n",
      "Epoch [2020/5000], Loss: 0.5716\n",
      "Epoch [2030/5000], Loss: 0.5714\n",
      "Epoch [2040/5000], Loss: 0.5712\n",
      "Epoch [2050/5000], Loss: 0.5711\n",
      "Epoch [2060/5000], Loss: 0.5709\n",
      "Epoch [2070/5000], Loss: 0.5707\n",
      "Epoch [2080/5000], Loss: 0.5705\n",
      "Epoch [2090/5000], Loss: 0.5704\n",
      "Epoch [2100/5000], Loss: 0.5702\n",
      "Epoch [2110/5000], Loss: 0.5700\n",
      "Epoch [2120/5000], Loss: 0.5699\n",
      "Epoch [2130/5000], Loss: 0.5697\n",
      "Epoch [2140/5000], Loss: 0.5695\n",
      "Epoch [2150/5000], Loss: 0.5694\n",
      "Epoch [2160/5000], Loss: 0.5692\n",
      "Epoch [2170/5000], Loss: 0.5691\n",
      "Epoch [2180/5000], Loss: 0.5689\n",
      "Epoch [2190/5000], Loss: 0.5688\n",
      "Epoch [2200/5000], Loss: 0.5686\n",
      "Epoch [2210/5000], Loss: 0.5685\n",
      "Epoch [2220/5000], Loss: 0.5683\n",
      "Epoch [2230/5000], Loss: 0.5682\n",
      "Epoch [2240/5000], Loss: 0.5680\n",
      "Epoch [2250/5000], Loss: 0.5679\n",
      "Epoch [2260/5000], Loss: 0.5678\n",
      "Epoch [2270/5000], Loss: 0.5676\n",
      "Epoch [2280/5000], Loss: 0.5675\n",
      "Epoch [2290/5000], Loss: 0.5674\n",
      "Epoch [2300/5000], Loss: 0.5673\n",
      "Epoch [2310/5000], Loss: 0.5671\n",
      "Epoch [2320/5000], Loss: 0.5670\n",
      "Epoch [2330/5000], Loss: 0.5669\n",
      "Epoch [2340/5000], Loss: 0.5668\n",
      "Epoch [2350/5000], Loss: 0.5666\n",
      "Epoch [2360/5000], Loss: 0.5665\n",
      "Epoch [2370/5000], Loss: 0.5664\n",
      "Epoch [2380/5000], Loss: 0.5663\n",
      "Epoch [2390/5000], Loss: 0.5662\n",
      "Epoch [2400/5000], Loss: 0.5660\n",
      "Epoch [2410/5000], Loss: 0.5659\n",
      "Epoch [2420/5000], Loss: 0.5658\n",
      "Epoch [2430/5000], Loss: 0.5657\n",
      "Epoch [2440/5000], Loss: 0.5656\n",
      "Epoch [2450/5000], Loss: 0.5655\n",
      "Epoch [2460/5000], Loss: 0.5654\n",
      "Epoch [2470/5000], Loss: 0.5653\n",
      "Epoch [2480/5000], Loss: 0.5652\n",
      "Epoch [2490/5000], Loss: 0.5651\n",
      "Epoch [2500/5000], Loss: 0.5650\n",
      "Epoch [2510/5000], Loss: 0.5648\n",
      "Epoch [2520/5000], Loss: 0.5647\n",
      "Epoch [2530/5000], Loss: 0.5647\n",
      "Epoch [2540/5000], Loss: 0.5646\n",
      "Epoch [2550/5000], Loss: 0.5645\n",
      "Epoch [2560/5000], Loss: 0.5644\n",
      "Epoch [2570/5000], Loss: 0.5643\n",
      "Epoch [2580/5000], Loss: 0.5642\n",
      "Epoch [2590/5000], Loss: 0.5641\n",
      "Epoch [2600/5000], Loss: 0.5640\n",
      "Epoch [2610/5000], Loss: 0.5639\n",
      "Epoch [2620/5000], Loss: 0.5638\n",
      "Epoch [2630/5000], Loss: 0.5638\n",
      "Epoch [2640/5000], Loss: 0.5637\n",
      "Epoch [2650/5000], Loss: 0.5636\n",
      "Epoch [2660/5000], Loss: 0.5635\n",
      "Epoch [2670/5000], Loss: 0.5634\n",
      "Epoch [2680/5000], Loss: 0.5633\n",
      "Epoch [2690/5000], Loss: 0.5633\n",
      "Epoch [2700/5000], Loss: 0.5632\n",
      "Epoch [2710/5000], Loss: 0.5631\n",
      "Epoch [2720/5000], Loss: 0.5630\n",
      "Epoch [2730/5000], Loss: 0.5630\n",
      "Epoch [2740/5000], Loss: 0.5629\n",
      "Epoch [2750/5000], Loss: 0.5628\n",
      "Epoch [2760/5000], Loss: 0.5627\n",
      "Epoch [2770/5000], Loss: 0.5627\n",
      "Epoch [2780/5000], Loss: 0.5626\n",
      "Epoch [2790/5000], Loss: 0.5625\n",
      "Epoch [2800/5000], Loss: 0.5624\n",
      "Epoch [2810/5000], Loss: 0.5624\n",
      "Epoch [2820/5000], Loss: 0.5623\n",
      "Epoch [2830/5000], Loss: 0.5622\n",
      "Epoch [2840/5000], Loss: 0.5621\n",
      "Epoch [2850/5000], Loss: 0.5621\n",
      "Epoch [2860/5000], Loss: 0.5620\n",
      "Epoch [2870/5000], Loss: 0.5619\n",
      "Epoch [2880/5000], Loss: 0.5619\n",
      "Epoch [2890/5000], Loss: 0.5618\n",
      "Epoch [2900/5000], Loss: 0.5617\n",
      "Epoch [2910/5000], Loss: 0.5617\n",
      "Epoch [2920/5000], Loss: 0.5616\n",
      "Epoch [2930/5000], Loss: 0.5615\n",
      "Epoch [2940/5000], Loss: 0.5615\n",
      "Epoch [2950/5000], Loss: 0.5614\n",
      "Epoch [2960/5000], Loss: 0.5614\n",
      "Epoch [2970/5000], Loss: 0.5613\n",
      "Epoch [2980/5000], Loss: 0.5612\n",
      "Epoch [2990/5000], Loss: 0.5612\n",
      "Epoch [3000/5000], Loss: 0.5611\n",
      "Epoch [3010/5000], Loss: 0.5611\n",
      "Epoch [3020/5000], Loss: 0.5610\n",
      "Epoch [3030/5000], Loss: 0.5610\n",
      "Epoch [3040/5000], Loss: 0.5609\n",
      "Epoch [3050/5000], Loss: 0.5609\n",
      "Epoch [3060/5000], Loss: 0.5608\n",
      "Epoch [3070/5000], Loss: 0.5607\n",
      "Epoch [3080/5000], Loss: 0.5607\n",
      "Epoch [3090/5000], Loss: 0.5606\n",
      "Epoch [3100/5000], Loss: 0.5606\n",
      "Epoch [3110/5000], Loss: 0.5605\n",
      "Epoch [3120/5000], Loss: 0.5605\n",
      "Epoch [3130/5000], Loss: 0.5604\n",
      "Epoch [3140/5000], Loss: 0.5604\n",
      "Epoch [3150/5000], Loss: 0.5603\n",
      "Epoch [3160/5000], Loss: 0.5603\n",
      "Epoch [3170/5000], Loss: 0.5602\n",
      "Epoch [3180/5000], Loss: 0.5602\n",
      "Epoch [3190/5000], Loss: 0.5601\n",
      "Epoch [3200/5000], Loss: 0.5601\n",
      "Epoch [3210/5000], Loss: 0.5600\n",
      "Epoch [3220/5000], Loss: 0.5600\n",
      "Epoch [3230/5000], Loss: 0.5599\n",
      "Epoch [3240/5000], Loss: 0.5599\n",
      "Epoch [3250/5000], Loss: 0.5598\n",
      "Epoch [3260/5000], Loss: 0.5598\n",
      "Epoch [3270/5000], Loss: 0.5597\n",
      "Epoch [3280/5000], Loss: 0.5597\n",
      "Epoch [3290/5000], Loss: 0.5597\n",
      "Epoch [3300/5000], Loss: 0.5596\n",
      "Epoch [3310/5000], Loss: 0.5596\n",
      "Epoch [3320/5000], Loss: 0.5595\n",
      "Epoch [3330/5000], Loss: 0.5595\n",
      "Epoch [3340/5000], Loss: 0.5594\n",
      "Epoch [3350/5000], Loss: 0.5594\n",
      "Epoch [3360/5000], Loss: 0.5594\n",
      "Epoch [3370/5000], Loss: 0.5593\n",
      "Epoch [3380/5000], Loss: 0.5593\n",
      "Epoch [3390/5000], Loss: 0.5592\n",
      "Epoch [3400/5000], Loss: 0.5592\n",
      "Epoch [3410/5000], Loss: 0.5591\n",
      "Epoch [3420/5000], Loss: 0.5591\n",
      "Epoch [3430/5000], Loss: 0.5591\n",
      "Epoch [3440/5000], Loss: 0.5590\n",
      "Epoch [3450/5000], Loss: 0.5590\n",
      "Epoch [3460/5000], Loss: 0.5589\n",
      "Epoch [3470/5000], Loss: 0.5589\n",
      "Epoch [3480/5000], Loss: 0.5589\n",
      "Epoch [3490/5000], Loss: 0.5588\n",
      "Epoch [3500/5000], Loss: 0.5588\n",
      "Epoch [3510/5000], Loss: 0.5587\n",
      "Epoch [3520/5000], Loss: 0.5587\n",
      "Epoch [3530/5000], Loss: 0.5587\n",
      "Epoch [3540/5000], Loss: 0.5586\n",
      "Epoch [3550/5000], Loss: 0.5586\n",
      "Epoch [3560/5000], Loss: 0.5586\n",
      "Epoch [3570/5000], Loss: 0.5585\n",
      "Epoch [3580/5000], Loss: 0.5585\n",
      "Epoch [3590/5000], Loss: 0.5585\n",
      "Epoch [3600/5000], Loss: 0.5584\n",
      "Epoch [3610/5000], Loss: 0.5584\n",
      "Epoch [3620/5000], Loss: 0.5583\n",
      "Epoch [3630/5000], Loss: 0.5583\n",
      "Epoch [3640/5000], Loss: 0.5583\n",
      "Epoch [3650/5000], Loss: 0.5582\n",
      "Epoch [3660/5000], Loss: 0.5582\n",
      "Epoch [3670/5000], Loss: 0.5582\n",
      "Epoch [3680/5000], Loss: 0.5581\n",
      "Epoch [3690/5000], Loss: 0.5581\n",
      "Epoch [3700/5000], Loss: 0.5581\n",
      "Epoch [3710/5000], Loss: 0.5580\n",
      "Epoch [3720/5000], Loss: 0.5580\n",
      "Epoch [3730/5000], Loss: 0.5580\n",
      "Epoch [3740/5000], Loss: 0.5580\n",
      "Epoch [3750/5000], Loss: 0.5579\n",
      "Epoch [3760/5000], Loss: 0.5579\n",
      "Epoch [3770/5000], Loss: 0.5579\n",
      "Epoch [3780/5000], Loss: 0.5578\n",
      "Epoch [3790/5000], Loss: 0.5578\n",
      "Epoch [3800/5000], Loss: 0.5578\n",
      "Epoch [3810/5000], Loss: 0.5577\n",
      "Epoch [3820/5000], Loss: 0.5577\n",
      "Epoch [3830/5000], Loss: 0.5577\n",
      "Epoch [3840/5000], Loss: 0.5576\n",
      "Epoch [3850/5000], Loss: 0.5576\n",
      "Epoch [3860/5000], Loss: 0.5576\n",
      "Epoch [3870/5000], Loss: 0.5576\n",
      "Epoch [3880/5000], Loss: 0.5575\n",
      "Epoch [3890/5000], Loss: 0.5575\n",
      "Epoch [3900/5000], Loss: 0.5575\n",
      "Epoch [3910/5000], Loss: 0.5574\n",
      "Epoch [3920/5000], Loss: 0.5574\n",
      "Epoch [3930/5000], Loss: 0.5574\n",
      "Epoch [3940/5000], Loss: 0.5574\n",
      "Epoch [3950/5000], Loss: 0.5573\n",
      "Epoch [3960/5000], Loss: 0.5573\n",
      "Epoch [3970/5000], Loss: 0.5573\n",
      "Epoch [3980/5000], Loss: 0.5572\n",
      "Epoch [3990/5000], Loss: 0.5572\n",
      "Epoch [4000/5000], Loss: 0.5572\n",
      "Epoch [4010/5000], Loss: 0.5571\n",
      "Epoch [4020/5000], Loss: 0.5571\n",
      "Epoch [4030/5000], Loss: 0.5571\n",
      "Epoch [4040/5000], Loss: 0.5571\n",
      "Epoch [4050/5000], Loss: 0.5570\n",
      "Epoch [4060/5000], Loss: 0.5570\n",
      "Epoch [4070/5000], Loss: 0.5570\n",
      "Epoch [4080/5000], Loss: 0.5569\n",
      "Epoch [4090/5000], Loss: 0.5569\n",
      "Epoch [4100/5000], Loss: 0.5569\n",
      "Epoch [4110/5000], Loss: 0.5569\n",
      "Epoch [4120/5000], Loss: 0.5568\n",
      "Epoch [4130/5000], Loss: 0.5568\n",
      "Epoch [4140/5000], Loss: 0.5568\n",
      "Epoch [4150/5000], Loss: 0.5568\n",
      "Epoch [4160/5000], Loss: 0.5567\n",
      "Epoch [4170/5000], Loss: 0.5567\n",
      "Epoch [4180/5000], Loss: 0.5567\n",
      "Epoch [4190/5000], Loss: 0.5567\n",
      "Epoch [4200/5000], Loss: 0.5566\n",
      "Epoch [4210/5000], Loss: 0.5566\n",
      "Epoch [4220/5000], Loss: 0.5566\n",
      "Epoch [4230/5000], Loss: 0.5566\n",
      "Epoch [4240/5000], Loss: 0.5565\n",
      "Epoch [4250/5000], Loss: 0.5565\n",
      "Epoch [4260/5000], Loss: 0.5565\n",
      "Epoch [4270/5000], Loss: 0.5565\n",
      "Epoch [4280/5000], Loss: 0.5564\n",
      "Epoch [4290/5000], Loss: 0.5564\n",
      "Epoch [4300/5000], Loss: 0.5564\n",
      "Epoch [4310/5000], Loss: 0.5564\n",
      "Epoch [4320/5000], Loss: 0.5563\n",
      "Epoch [4330/5000], Loss: 0.5563\n",
      "Epoch [4340/5000], Loss: 0.5563\n",
      "Epoch [4350/5000], Loss: 0.5563\n",
      "Epoch [4360/5000], Loss: 0.5563\n",
      "Epoch [4370/5000], Loss: 0.5562\n",
      "Epoch [4380/5000], Loss: 0.5562\n",
      "Epoch [4390/5000], Loss: 0.5562\n",
      "Epoch [4400/5000], Loss: 0.5562\n",
      "Epoch [4410/5000], Loss: 0.5561\n",
      "Epoch [4420/5000], Loss: 0.5561\n",
      "Epoch [4430/5000], Loss: 0.5561\n",
      "Epoch [4440/5000], Loss: 0.5561\n",
      "Epoch [4450/5000], Loss: 0.5561\n",
      "Epoch [4460/5000], Loss: 0.5560\n",
      "Epoch [4470/5000], Loss: 0.5560\n",
      "Epoch [4480/5000], Loss: 0.5560\n",
      "Epoch [4490/5000], Loss: 0.5560\n",
      "Epoch [4500/5000], Loss: 0.5560\n",
      "Epoch [4510/5000], Loss: 0.5559\n",
      "Epoch [4520/5000], Loss: 0.5559\n",
      "Epoch [4530/5000], Loss: 0.5559\n",
      "Epoch [4540/5000], Loss: 0.5559\n",
      "Epoch [4550/5000], Loss: 0.5559\n",
      "Epoch [4560/5000], Loss: 0.5558\n",
      "Epoch [4570/5000], Loss: 0.5558\n",
      "Epoch [4580/5000], Loss: 0.5558\n",
      "Epoch [4590/5000], Loss: 0.5558\n",
      "Epoch [4600/5000], Loss: 0.5558\n",
      "Epoch [4610/5000], Loss: 0.5557\n",
      "Epoch [4620/5000], Loss: 0.5557\n",
      "Epoch [4630/5000], Loss: 0.5557\n",
      "Epoch [4640/5000], Loss: 0.5557\n",
      "Epoch [4650/5000], Loss: 0.5557\n",
      "Epoch [4660/5000], Loss: 0.5556\n",
      "Epoch [4670/5000], Loss: 0.5556\n",
      "Epoch [4680/5000], Loss: 0.5556\n",
      "Epoch [4690/5000], Loss: 0.5556\n",
      "Epoch [4700/5000], Loss: 0.5556\n",
      "Epoch [4710/5000], Loss: 0.5555\n",
      "Epoch [4720/5000], Loss: 0.5555\n",
      "Epoch [4730/5000], Loss: 0.5555\n",
      "Epoch [4740/5000], Loss: 0.5555\n",
      "Epoch [4750/5000], Loss: 0.5555\n",
      "Epoch [4760/5000], Loss: 0.5555\n",
      "Epoch [4770/5000], Loss: 0.5554\n",
      "Epoch [4780/5000], Loss: 0.5554\n",
      "Epoch [4790/5000], Loss: 0.5554\n",
      "Epoch [4800/5000], Loss: 0.5554\n",
      "Epoch [4810/5000], Loss: 0.5554\n",
      "Epoch [4820/5000], Loss: 0.5554\n",
      "Epoch [4830/5000], Loss: 0.5553\n",
      "Epoch [4840/5000], Loss: 0.5553\n",
      "Epoch [4850/5000], Loss: 0.5553\n",
      "Epoch [4860/5000], Loss: 0.5553\n",
      "Epoch [4870/5000], Loss: 0.5553\n",
      "Epoch [4880/5000], Loss: 0.5553\n",
      "Epoch [4890/5000], Loss: 0.5552\n",
      "Epoch [4900/5000], Loss: 0.5552\n",
      "Epoch [4910/5000], Loss: 0.5552\n",
      "Epoch [4920/5000], Loss: 0.5552\n",
      "Epoch [4930/5000], Loss: 0.5552\n",
      "Epoch [4940/5000], Loss: 0.5552\n",
      "Epoch [4950/5000], Loss: 0.5551\n",
      "Epoch [4960/5000], Loss: 0.5551\n",
      "Epoch [4970/5000], Loss: 0.5551\n",
      "Epoch [4980/5000], Loss: 0.5551\n",
      "Epoch [4990/5000], Loss: 0.5551\n",
      "Epoch [5000/5000], Loss: 0.5551\n"
     ]
    }
   ],
   "source": [
    "# Di chuyển mô hình và dữ liệu vào GPU nếu có sẵn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "# Xác định hàm mất mát và thuật toán tối ưu hóa\n",
    "criterion = nn.MSELoss()  # Hàm mất mát Mean Squared Error (MSE) cho bài toán hồi quy\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Thuật toán tối ưu hóa SGD với learning rate 0.01\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Tính toán loss\n",
    "    loss = criterion(outputs, y_train.unsqueeze(1))\n",
    "\n",
    "    # Backward pass và cập nhật trọng số\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # In ra thông tin về quá trình huấn luyện\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.to(\"cpu\")\n",
    "X_train = X_train.to(\"cpu\")\n",
    "y_train = y_train.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) trên tập test: 90.08%\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình trên tập test bằng mean squared error (MSE)\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    y_test_tensor = y_test.unsqueeze(1)[:outputs.size(0)]  # Đảm bảo kích thước của y_test phù hợp với outputs\n",
    "    mse = criterion(outputs, y_test_tensor)\n",
    "    # In ra kết quả MSE dạng phần trăm\n",
    "    print(f'Mean Squared Error (MSE) trên tập test: {mse.item()*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for test_data\n",
    "\n",
    "test_data = pd.read_csv(os.path.join(folder_path, 'test.csv'))\n",
    "# Convert the test_data to same format as train_data\n",
    "test_data['MSSubClass'] = test_data['MSSubClass'].astype(str)\n",
    "\n",
    "# Save ID column for submission\n",
    "test_data_ID = test_data['Id']\n",
    "test_data.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Label encoding for categorical columns, keep dataframes type but test_data doesn't have SalePrice column\n",
    "# So we will build new categorical_cols for test_data\n",
    "selected_features2 = selected_features.copy()\n",
    "selected_features2.remove('SalePrice')\n",
    "for col in selected_features2:\n",
    "    test_data[col] = label_encoder.fit_transform(test_data[col])\n",
    "\n",
    "# Get the selected features to new dataframe\n",
    "prelast_test_data = test_data[selected_features2]\n",
    "\n",
    "# Fit scaler trên tập test\n",
    "scaler.fit(prelast_test_data)\n",
    "\n",
    "# Transform tập test\n",
    "last_test_data = scaler.transform(prelast_test_data)\n",
    "\n",
    "# Convert the numpy array to torch tensor\n",
    "last_test_data = torch.tensor(last_test_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã hoàn thành xuất file D:\\FPTUni\\SP24\\ADY201m\\Lab05\\ADY201m_Lab05_SE183256\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test set, last_test_data is the scaled test set and it is a tensor now\n",
    "X_test = last_test_data\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(X_test)\n",
    "\n",
    "# Chuyển kết quả dự đoán về dạng numpy array\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "# Chuyển kết quả dự đoán về dạng chuẩn\n",
    "y_pred = scaler_output.inverse_transform(y_pred)\n",
    "\n",
    "# Làm tròn kết quả dự đoán\n",
    "y_pred = y_pred.round()\n",
    "\n",
    "# Tạo dataframe chứa kết quả dự đoán\n",
    "submission = pd.DataFrame({'Id': test_data_ID, 'SalePrice': y_pred.flatten()})\n",
    "\n",
    "# Xuất dataframe ra file CSV\n",
    "submission.to_csv(submit_file, index=False)\n",
    "\n",
    "# In ra thông báo hoàn thành\n",
    "print(f\"Đã hoàn thành xuất file {submit_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra kết quả dự đoán bằng tập X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
